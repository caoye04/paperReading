# CodeMind: A Framework to Challenge Large Language Models for Code Reasoning

> Univ. of Illinois at Urbana-Champaign[（UIUC：cs rank 2nd in the world during 10 years）](https://csrankings.org/#/index?all&world)
>
> [Changshu Liu](https://scholar.google.com/citations?user=cU2ynnIAAAAJ&hl=en)：电子科技大学（cd） - Columbia University - UIUC 
>
> 其有三篇工作，主要和代码分析相关的就是CodeMind和SarGaM（另一个是自监督图分析，天跨越好大）
>
> **CodeMind: A Framework to Challenge Large Language Models for Code Reasoning（2024/2写完的还没发出来，但是被引已经30了，感觉主要原因是和另一个UIUC的刘投的neurIps的工作[`EvalPlus`](https://arxiv.org/abs/2305.01210)相似度比较高）**
>
> 本质上就提出一个新的评估框架，之前评估LLMs的diamond能力，只考虑了代码生成能力，这里考虑评估：独立执行推理（IER）、依赖执行推理（DER）和规范推理（SR），前两种任务评估模型预测任意代码或模型可以正确合成的代码的执行输出的能力。第三种任务评估LLMs实现指定预期行为的程度。
>
> 其实就是一个评估LLMs的代码推理能力工具和框架。
>
> 我感觉对我们研究有作用的点一个是可以用于参考之后论文中result的设计、评估方法的设计。甚至可以直接import CondeMind评估。
>
> **Automated code editing with search-generate-modify**（发在了IEEE/ACM）
>
> 这个主要是提了SARGAM工具，提出一个混合方法（代码搜索、生成和修改）来更好地合成代码编辑 Bug fixing, Automated Program Repair, Editbased Neural Network
>
> **Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation**
>
> 和CM很像，也是提出一个LLMs代码分析框架，但这篇写的更早，是2023/5写完的。
>
> 我怀疑他们是一个实验室的。
>
> 发在了NeurIps上，被引量900+
>
> 提出了更严格的代码合成评估框架；通过自动化方法大幅扩展测试用例（将HumanEval基准测试的测试用例扩展了80倍，创建了HumanEval+）
>
> 表明现有的代码合成评估结果不能准确反映LLMs的真实性能；开创了通过自动化测试改进编程基准的新方向；开源了工具、增强数据集和LLM生成的代码